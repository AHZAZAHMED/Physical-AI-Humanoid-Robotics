"use strict";(globalThis.webpackChunkphysical_ai_textbook=globalThis.webpackChunkphysical_ai_textbook||[]).push([[260],{6148:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>m,frontMatter:()=>r,metadata:()=>o,toc:()=>l});const o=JSON.parse('{"id":"module-4/voice-to-action","title":"Voice to Action","description":"Voice to Action (VTA) systems enable humanoid robots to understand spoken commands and convert them into executable robotic actions. This module covers implementing voice recognition, natural language processing, and action execution for humanoid robots.","source":"@site/docs/module-4/voice-to-action.md","sourceDirName":"module-4","slug":"/module-4/voice-to-action","permalink":"/Physical-AI-Humanoid-Robotics/docs/module-4/voice-to-action","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedAt":1765393240000,"frontMatter":{"title":"Voice to Action"},"sidebar":"tutorialSidebar","previous":{"title":"Nav2 and Bipedal Movement","permalink":"/Physical-AI-Humanoid-Robotics/docs/module-3/nav2-and-bipedal-movement"},"next":{"title":"Cognitive Planning","permalink":"/Physical-AI-Humanoid-Robotics/docs/module-4/cognitive-planning"}}');var t=i(4848),s=i(8453);const r={title:"Voice to Action"},a="Voice to Action",c={},l=[{value:"Introduction to Voice to Action Systems",id:"introduction-to-voice-to-action-systems",level:2},{value:"Architecture of VTA Systems",id:"architecture-of-vta-systems",level:2},{value:"Processing Pipeline",id:"processing-pipeline",level:3},{value:"System Components",id:"system-components",level:3},{value:"Speech Recognition for Robotics",id:"speech-recognition-for-robotics",level:2},{value:"Real-time Speech Recognition",id:"real-time-speech-recognition",level:3},{value:"Noise Reduction Techniques",id:"noise-reduction-techniques",level:3},{value:"Natural Language Understanding",id:"natural-language-understanding",level:2},{value:"Intent Recognition",id:"intent-recognition",level:3},{value:"Common Robot Commands",id:"common-robot-commands",level:3},{value:"Action Mapping and Execution",id:"action-mapping-and-execution",level:2},{value:"Command to Action Mapping",id:"command-to-action-mapping",level:3},{value:"Integration with ROS 2",id:"integration-with-ros-2",level:2},{value:"Voice Command Node",id:"voice-command-node",level:3},{value:"OpenAI Whisper Integration",id:"openai-whisper-integration",level:2},{value:"Using Whisper for Speech Recognition",id:"using-whisper-for-speech-recognition",level:3},{value:"Context and State Management",id:"context-and-state-management",level:2},{value:"Maintaining Conversation Context",id:"maintaining-conversation-context",level:3},{value:"Speech Synthesis and Feedback",id:"speech-synthesis-and-feedback",level:2},{value:"Text-to-Speech for Robot Responses",id:"text-to-speech-for-robot-responses",level:3},{value:"Multi-modal Integration",id:"multi-modal-integration",level:2},{value:"Combining Voice with Visual Information",id:"combining-voice-with-visual-information",level:3},{value:"Performance and Robustness",id:"performance-and-robustness",level:2},{value:"Handling Noisy Environments",id:"handling-noisy-environments",level:3},{value:"Error Handling and Recovery",id:"error-handling-and-recovery",level:3},{value:"Best Practices",id:"best-practices",level:2},{value:"Learning Objectives",id:"learning-objectives",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"voice-to-action",children:"Voice to Action"})}),"\n",(0,t.jsx)(n.p,{children:"Voice to Action (VTA) systems enable humanoid robots to understand spoken commands and convert them into executable robotic actions. This module covers implementing voice recognition, natural language processing, and action execution for humanoid robots."}),"\n",(0,t.jsx)(n.h2,{id:"introduction-to-voice-to-action-systems",children:"Introduction to Voice to Action Systems"}),"\n",(0,t.jsx)(n.p,{children:"Voice to Action systems for humanoid robots involve:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Speech Recognition"}),": Converting speech to text"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Natural Language Understanding"}),": Interpreting user intent"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Action Mapping"}),": Converting commands to robotic actions"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Speech Synthesis"}),": Providing verbal feedback"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Context Awareness"}),": Understanding commands in context"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"architecture-of-vta-systems",children:"Architecture of VTA Systems"}),"\n",(0,t.jsx)(n.h3,{id:"processing-pipeline",children:"Processing Pipeline"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Audio Capture"}),": Microphone array processing"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Speech Recognition"}),": ASR (Automatic Speech Recognition)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Intent Classification"}),": Understanding user intent"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Entity Extraction"}),": Identifying relevant parameters"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Action Planning"}),": Converting to executable actions"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Action Execution"}),": Performing the requested actions"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Feedback Generation"}),": Verbal or non-verbal feedback"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"system-components",children:"System Components"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Audio Processing"}),": Noise reduction and beamforming"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Language Model"}),": Understanding natural language"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Action Database"}),": Mapping commands to actions"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Context Manager"}),": Maintaining conversation state"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"speech-recognition-for-robotics",children:"Speech Recognition for Robotics"}),"\n",(0,t.jsx)(n.h3,{id:"real-time-speech-recognition",children:"Real-time Speech Recognition"}),"\n",(0,t.jsx)(n.p,{children:"Robots require real-time speech recognition with low latency:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"import speech_recognition as sr\nimport threading\n\nclass RobotSpeechRecognizer:\n    def __init__(self):\n        self.recognizer = sr.Recognizer()\n        self.microphone = sr.Microphone()\n        self.is_listening = False\n\n        # Adjust for ambient noise\n        with self.microphone as source:\n            self.recognizer.adjust_for_ambient_noise(source)\n\n    def start_listening(self, callback):\n        self.is_listening = True\n        threading.Thread(target=self._listen_loop, args=(callback,)).start()\n\n    def _listen_loop(self, callback):\n        with self.microphone as source:\n            while self.is_listening:\n                try:\n                    audio = self.recognizer.listen(source, timeout=1)\n                    text = self.recognizer.recognize_google(audio)\n                    callback(text)\n                except sr.WaitTimeoutError:\n                    continue\n                except sr.UnknownValueError:\n                    continue\n"})}),"\n",(0,t.jsx)(n.h3,{id:"noise-reduction-techniques",children:"Noise Reduction Techniques"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Beamforming"}),": Focus on speaker's voice"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Echo Cancellation"}),": Remove robot's own speech"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Noise Suppression"}),": Filter background noise"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Voice Activity Detection"}),": Detect speech presence"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"natural-language-understanding",children:"Natural Language Understanding"}),"\n",(0,t.jsx)(n.h3,{id:"intent-recognition",children:"Intent Recognition"}),"\n",(0,t.jsx)(n.p,{children:"Identifying the user's intent from spoken commands:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"import spacy\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\n\nclass IntentClassifier:\n    def __init__(self):\n        self.nlp = spacy.load(\"en_core_web_sm\")\n        self.vectorizer = TfidfVectorizer()\n        self.classifier = MultinomialNB()\n        self.intents = {}\n\n    def extract_intent(self, text):\n        # Preprocess text\n        doc = self.nlp(text.lower())\n        tokens = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct]\n\n        # Extract features\n        features = self.vectorizer.transform([' '.join(tokens)])\n\n        # Predict intent\n        intent = self.classifier.predict(features)[0]\n        confidence = max(self.classifier.predict_proba(features)[0])\n\n        return intent, confidence\n"})}),"\n",(0,t.jsx)(n.h3,{id:"common-robot-commands",children:"Common Robot Commands"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Navigation"}),': "Go to the kitchen", "Move forward"']}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Manipulation"}),': "Pick up the red ball", "Open the door"']}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Interaction"}),': "What\'s your name?", "Tell me a joke"']}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Control"}),': "Stop", "Wait", "Follow me"']}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"action-mapping-and-execution",children:"Action Mapping and Execution"}),"\n",(0,t.jsx)(n.h3,{id:"command-to-action-mapping",children:"Command to Action Mapping"}),"\n",(0,t.jsx)(n.p,{children:"Mapping natural language commands to robotic actions:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"class ActionMapper:\n    def __init__(self):\n        self.action_map = {\n            'move_to': ['go to', 'move to', 'navigate to', 'walk to'],\n            'pick_up': ['pick up', 'grasp', 'take', 'get'],\n            'place': ['place', 'put', 'set down'],\n            'follow': ['follow', 'come with me', 'accompany'],\n            'stop': ['stop', 'halt', 'freeze'],\n            'answer': ['what', 'how', 'why', 'tell me', 'explain']\n        }\n\n    def map_command(self, command):\n        command_lower = command.lower()\n\n        for action, keywords in self.action_map.items():\n            for keyword in keywords:\n                if keyword in command_lower:\n                    # Extract parameters using NLP\n                    params = self.extract_parameters(command_lower, action)\n                    return action, params\n\n        return 'unknown', {}\n\n    def extract_parameters(self, command, action):\n        # Extract entities based on action type\n        if action == 'move_to':\n            # Look for location entities\n            locations = ['kitchen', 'living room', 'bedroom', 'office', 'bathroom']\n            for loc in locations:\n                if loc in command:\n                    return {'location': loc}\n        elif action == 'pick_up':\n            # Look for object entities\n            objects = ['ball', 'cup', 'book', 'phone', 'keys']\n            for obj in objects:\n                if obj in command:\n                    color = self.extract_color(command)\n                    return {'object': obj, 'color': color}\n\n        return {}\n\n    def extract_color(self, command):\n        colors = ['red', 'blue', 'green', 'yellow', 'black', 'white']\n        for color in colors:\n            if color in command:\n                return color\n        return None\n"})}),"\n",(0,t.jsx)(n.h2,{id:"integration-with-ros-2",children:"Integration with ROS 2"}),"\n",(0,t.jsx)(n.h3,{id:"voice-command-node",children:"Voice Command Node"}),"\n",(0,t.jsx)(n.p,{children:"Creating a ROS 2 node for voice command processing:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom geometry_msgs.msg import Pose\nfrom sensor_msgs.msg import AudioData\n\nclass VoiceCommandNode(Node):\n    def __init__(self):\n        super().__init__('voice_command_node')\n\n        # Publishers\n        self.nav_goal_pub = self.create_publisher(Pose, 'navigation_goal', 10)\n        self.action_pub = self.create_publisher(String, 'robot_action', 10)\n\n        # Subscriber\n        self.voice_sub = self.create_subscription(\n            String, 'voice_commands', self.voice_callback, 10)\n\n        # Action mapper\n        self.action_mapper = ActionMapper()\n\n        self.get_logger().info('Voice Command Node initialized')\n\n    def voice_callback(self, msg):\n        command = msg.data\n        self.get_logger().info(f'Received command: {command}')\n\n        # Map command to action\n        action, params = self.action_mapper.map_command(command)\n\n        if action != 'unknown':\n            # Execute action\n            self.execute_action(action, params)\n        else:\n            self.get_logger().warn(f'Unknown command: {command}')\n\n    def execute_action(self, action, params):\n        if action == 'move_to':\n            self.navigate_to_location(params['location'])\n        elif action == 'pick_up':\n            self.pick_up_object(params)\n        elif action == 'stop':\n            self.stop_robot()\n\n    def navigate_to_location(self, location):\n        # Publish navigation goal\n        goal = Pose()\n        # Set goal based on location map\n        self.nav_goal_pub.publish(goal)\n"})}),"\n",(0,t.jsx)(n.h2,{id:"openai-whisper-integration",children:"OpenAI Whisper Integration"}),"\n",(0,t.jsx)(n.h3,{id:"using-whisper-for-speech-recognition",children:"Using Whisper for Speech Recognition"}),"\n",(0,t.jsx)(n.p,{children:"OpenAI Whisper provides robust speech recognition capabilities:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import whisper\nimport torch\nimport pyaudio\nimport wave\nimport numpy as np\n\nclass WhisperVoiceProcessor:\n    def __init__(self, model_size="base"):\n        # Load Whisper model\n        self.model = whisper.load_model(model_size)\n        self.audio_format = pyaudio.paInt16\n        self.channels = 1\n        self.rate = 16000\n        self.chunk = 1024\n\n    def record_audio(self, duration=5):\n        p = pyaudio.PyAudio()\n\n        stream = p.open(format=self.audio_format,\n                       channels=self.channels,\n                       rate=self.rate,\n                       input=True,\n                       frames_per_buffer=self.chunk)\n\n        self.get_logger().info("Recording...")\n        frames = []\n\n        for i in range(0, int(self.rate / self.chunk * duration)):\n            data = stream.read(self.chunk)\n            frames.append(data)\n\n        self.get_logger().info("Recording finished")\n\n        stream.stop_stream()\n        stream.close()\n        p.terminate()\n\n        # Save to WAV file\n        filename = "temp_recording.wav"\n        wf = wave.open(filename, \'wb\')\n        wf.setnchannels(self.channels)\n        wf.setsampwidth(p.get_sample_size(self.audio_format))\n        wf.setframerate(self.rate)\n        wf.writeframes(b\'\'.join(frames))\n        wf.close()\n\n        return filename\n\n    def transcribe_audio(self, audio_file):\n        # Load audio file\n        audio = whisper.load_audio(audio_file)\n        audio = whisper.pad_or_trim(audio)\n\n        # Convert to log-Mel spectrogram\n        mel = whisper.log_mel_spectrogram(audio).to(self.model.device)\n\n        # Decode\n        options = whisper.DecodingOptions()\n        result = whisper.decode(self.model, mel, options)\n\n        return result.text\n'})}),"\n",(0,t.jsx)(n.h2,{id:"context-and-state-management",children:"Context and State Management"}),"\n",(0,t.jsx)(n.h3,{id:"maintaining-conversation-context",children:"Maintaining Conversation Context"}),"\n",(0,t.jsx)(n.p,{children:"Handling multi-turn conversations and context:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"class ConversationContext:\n    def __init__(self):\n        self.context = {}\n        self.history = []\n        self.current_task = None\n        self.entities = {}\n\n    def update_context(self, user_input, robot_response):\n        self.history.append({\n            'user': user_input,\n            'robot': robot_response,\n            'timestamp': time.time()\n        })\n\n        # Update entities mentioned\n        self.extract_entities(user_input)\n\n    def extract_entities(self, text):\n        # Extract and store entities from text\n        doc = self.nlp(text)\n        for ent in doc.ents:\n            self.entities[ent.text] = ent.label_\n\n    def get_context_for_intent(self, intent):\n        # Provide context relevant to the current intent\n        if intent == 'follow':\n            return {'previous_location': self.context.get('last_location')}\n        elif intent == 'pick_up':\n            return {'available_objects': self.entities}\n        return self.context\n"})}),"\n",(0,t.jsx)(n.h2,{id:"speech-synthesis-and-feedback",children:"Speech Synthesis and Feedback"}),"\n",(0,t.jsx)(n.h3,{id:"text-to-speech-for-robot-responses",children:"Text-to-Speech for Robot Responses"}),"\n",(0,t.jsx)(n.p,{children:"Providing verbal feedback to users:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"import pyttsx3\nimport threading\n\nclass RobotSpeechSynthesizer:\n    def __init__(self):\n        self.engine = pyttsx3.init()\n\n        # Configure voice properties\n        voices = self.engine.getProperty('voices')\n        self.engine.setProperty('voice', voices[0].id)  # Choose voice\n        self.engine.setProperty('rate', 150)  # Speed of speech\n        self.engine.setProperty('volume', 0.9)  # Volume level\n\n    def speak(self, text, blocking=False):\n        if blocking:\n            self.engine.say(text)\n            self.engine.runAndWait()\n        else:\n            # Speak in a separate thread\n            thread = threading.Thread(target=self._speak_thread, args=(text,))\n            thread.start()\n\n    def _speak_thread(self, text):\n        self.engine.say(text)\n        self.engine.runAndWait()\n\n    def set_voice_properties(self, rate=None, volume=None, voice=None):\n        if rate is not None:\n            self.engine.setProperty('rate', rate)\n        if volume is not None:\n            self.engine.setProperty('volume', volume)\n        if voice is not None:\n            self.engine.setProperty('voice', voice)\n"})}),"\n",(0,t.jsx)(n.h2,{id:"multi-modal-integration",children:"Multi-modal Integration"}),"\n",(0,t.jsx)(n.h3,{id:"combining-voice-with-visual-information",children:"Combining Voice with Visual Information"}),"\n",(0,t.jsx)(n.p,{children:"Enhancing voice commands with visual context:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"class MultiModalVTA:\n    def __init__(self):\n        self.voice_processor = WhisperVoiceProcessor()\n        self.vision_processor = VisionProcessor()  # Custom vision module\n        self.action_mapper = ActionMapper()\n        self.context_manager = ConversationContext()\n\n    def process_multimodal_command(self, audio_file, visual_context=None):\n        # Transcribe audio\n        text = self.voice_processor.transcribe_audio(audio_file)\n\n        # Enhance with visual context if available\n        if visual_context:\n            text = self.enhance_with_visual_context(text, visual_context)\n\n        # Map to action\n        action, params = self.action_mapper.map_command(text)\n\n        # Execute with context\n        self.execute_contextual_action(action, params, visual_context)\n\n    def enhance_with_visual_context(self, text, visual_context):\n        # Add visual information to the command\n        # Example: \"Pick up the red ball\" -> \"Pick up the red ball on the table\"\n        if 'the' in text and visual_context.get('objects'):\n            # Add spatial information based on visual detection\n            enhanced_text = self.add_spatial_context(text, visual_context)\n            return enhanced_text\n        return text\n"})}),"\n",(0,t.jsx)(n.h2,{id:"performance-and-robustness",children:"Performance and Robustness"}),"\n",(0,t.jsx)(n.h3,{id:"handling-noisy-environments",children:"Handling Noisy Environments"}),"\n",(0,t.jsx)(n.p,{children:"Techniques for robust operation in challenging acoustic conditions:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Multiple Microphones"}),": Array processing for better signal"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Adaptive Filtering"}),": Adjust to changing noise conditions"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Confidence Scoring"}),": Verify recognition accuracy"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Fallback Mechanisms"}),": Alternative input methods"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"error-handling-and-recovery",children:"Error Handling and Recovery"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'def handle_recognition_error(self, error_type):\n    if error_type == "no_speech":\n        self.speak("I didn\'t hear anything. Could you repeat that?")\n    elif error_type == "unclear":\n        self.speak("I didn\'t understand. Could you rephrase that?")\n    elif error_type == "unknown_command":\n        self.speak("I don\'t know how to do that yet.")\n'})}),"\n",(0,t.jsx)(n.h2,{id:"best-practices",children:"Best Practices"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Privacy"}),": Secure handling of voice data"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Latency"}),": Optimize for real-time response"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Accuracy"}),": Validate recognition results"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Feedback"}),": Provide clear status to users"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Fallback"}),": Have alternative interaction methods"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,t.jsx)(n.p,{children:"After completing this module, you will be able to:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Implement speech recognition for humanoid robots"}),"\n",(0,t.jsx)(n.li,{children:"Design natural language understanding systems"}),"\n",(0,t.jsx)(n.li,{children:"Map voice commands to robotic actions"}),"\n",(0,t.jsx)(n.li,{children:"Integrate with ROS 2 and OpenAI Whisper"}),"\n",(0,t.jsx)(n.li,{children:"Handle multi-modal voice and visual commands"}),"\n",(0,t.jsx)(n.li,{children:"Optimize VTA systems for real-world deployment"}),"\n"]})]})}function m(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>r,x:()=>a});var o=i(6540);const t={},s=o.createContext(t);function r(e){const n=o.useContext(s);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:r(e.components),o.createElement(s.Provider,{value:n},e.children)}}}]);